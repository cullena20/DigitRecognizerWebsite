{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "x_test:  (10000, 28, 28)\n",
      "y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('x_train: ' + str(x_train.shape))\n",
    "print('y_train: ' + str(y_train.shape))\n",
    "print('x_test:  '  + str(x_test.shape))\n",
    "print('y_test:  '  + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tensorflow Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 14:46:42.359206: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2496 - accuracy: 0.9290 - val_loss: 0.1306 - val_accuracy: 0.9614\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.9669 - val_loss: 0.0952 - val_accuracy: 0.9708\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 2s 848us/step - loss: 0.0759 - accuracy: 0.9766 - val_loss: 0.0833 - val_accuracy: 0.9745\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0975 - val_accuracy: 0.9702\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 2s 997us/step - loss: 0.0434 - accuracy: 0.9866 - val_loss: 0.0731 - val_accuracy: 0.9775\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0348 - accuracy: 0.9893 - val_loss: 0.0742 - val_accuracy: 0.9765\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 2s 819us/step - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.0710 - val_accuracy: 0.9787\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 1s 771us/step - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.0742 - val_accuracy: 0.9790\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0189 - accuracy: 0.9942 - val_loss: 0.0771 - val_accuracy: 0.9785\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 1s 767us/step - loss: 0.0146 - accuracy: 0.9955 - val_loss: 0.0792 - val_accuracy: 0.9782\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.0810 - val_accuracy: 0.9775\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.1030 - val_accuracy: 0.9747\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 2s 836us/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0775 - val_accuracy: 0.9803\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0909 - val_accuracy: 0.9788\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 2s 869us/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.1040 - val_accuracy: 0.9757\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 2s 985us/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0883 - val_accuracy: 0.9790\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 1s 766us/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.0923 - val_accuracy: 0.9795\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 1s 751us/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0889 - val_accuracy: 0.9799\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 1s 797us/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0971 - val_accuracy: 0.9792\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.1005 - val_accuracy: 0.9789\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.1106 - val_accuracy: 0.9788\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.1196 - val_accuracy: 0.9763\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 1s 761us/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.1239 - val_accuracy: 0.9775\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 1s 799us/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.1202 - val_accuracy: 0.9792\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 2s 973us/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.1228 - val_accuracy: 0.9794\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.1262 - val_accuracy: 0.9780\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.1250 - val_accuracy: 0.9793\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.1328 - val_accuracy: 0.9777\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 2s 830us/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.1280 - val_accuracy: 0.9772\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 1s 751us/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.1306 - val_accuracy: 0.9792\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 1s 791us/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.1336 - val_accuracy: 0.9779\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 1s 798us/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.1362 - val_accuracy: 0.9789\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 1s 782us/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.1191 - val_accuracy: 0.9792\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 1s 776us/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.1236 - val_accuracy: 0.9790\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 2s 808us/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.1527 - val_accuracy: 0.9767\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 2s 928us/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.1459 - val_accuracy: 0.9772\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 2s 886us/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1437 - val_accuracy: 0.9795\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 2s 897us/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.1496 - val_accuracy: 0.9789\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 2s 936us/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.1468 - val_accuracy: 0.9801\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.1545 - val_accuracy: 0.9793\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.1318 - val_accuracy: 0.9802\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1498 - val_accuracy: 0.9789\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 1s 753us/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.1532 - val_accuracy: 0.9796\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 1s 764us/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.1382 - val_accuracy: 0.9789\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 1s 793us/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.1553 - val_accuracy: 0.9779\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 1s 756us/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.1375 - val_accuracy: 0.9810\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.1710 - val_accuracy: 0.9775\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 1s 750us/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.1808 - val_accuracy: 0.9775\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 1s 754us/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.1509 - val_accuracy: 0.9793\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 1s 782us/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.1728 - val_accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bfe3f390>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 461us/step - loss: 0.1728 - accuracy: 0.9771\n",
      "Test Loss: 0.173\n",
      "Test Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss:.3f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/simple_keras_nn/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"cnns/simple_keras_nn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 22s 46ms/step - loss: 0.3686 - accuracy: 0.8820 - val_loss: 0.0644 - val_accuracy: 0.9805\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.1172 - accuracy: 0.9642 - val_loss: 0.0396 - val_accuracy: 0.9870\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 26s 55ms/step - loss: 0.0884 - accuracy: 0.9743 - val_loss: 0.0347 - val_accuracy: 0.9895\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 26s 54ms/step - loss: 0.0748 - accuracy: 0.9774 - val_loss: 0.0301 - val_accuracy: 0.9904\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0670 - accuracy: 0.9803 - val_loss: 0.0286 - val_accuracy: 0.9909\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 22s 47ms/step - loss: 0.0600 - accuracy: 0.9820 - val_loss: 0.0258 - val_accuracy: 0.9913\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 19s 39ms/step - loss: 0.0550 - accuracy: 0.9829 - val_loss: 0.0238 - val_accuracy: 0.9923\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.0494 - accuracy: 0.9846 - val_loss: 0.0226 - val_accuracy: 0.9924\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 0.0477 - accuracy: 0.9850 - val_loss: 0.0233 - val_accuracy: 0.9930\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0444 - accuracy: 0.9866 - val_loss: 0.0225 - val_accuracy: 0.9922\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0225 - accuracy: 0.9922\n",
      "Test Loss: 0.02252611145377159\n",
      "Test Accuracy: 0.9922000169754028\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Load MNIST dataset and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "num_classes = 10\n",
    "\n",
    "# Create the model\n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn.add(MaxPooling2D((2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D((2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "\n",
    "cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = cnn.evaluate(x_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 15:17:51.475217: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,13,13,32]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-06-29 15:17:51.483759: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,5,5,64]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-06-29 15:17:51.492603: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-06-29 15:17:51.604271: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,13,13,32]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-06-29 15:17:51.627104: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,5,5,64]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-06-29 15:17:51.651415: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/cnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/cnn/assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save(\"models/cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'visualkeras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvisualkeras\u001b[39;00m\n\u001b[1;32m      3\u001b[0m visualkeras\u001b[38;5;241m.\u001b[39mlayered_view(model)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'visualkeras'"
     ]
    }
   ],
   "source": [
    "import visualkeras \n",
    "\n",
    "visualkeras.layered_view(model).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing On Drawn Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1120x738 at 0x2942F8C90>\n",
      "<PIL.Image.Image image mode=RGBA size=28x28 at 0x282353990>\n",
      "(28, 28, 4)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the original image\n",
    "original_image = Image.open(\"../images/my-drawing.png\")\n",
    "print(original_image)\n",
    "\n",
    "# Resize the image to 28x28 using bilinear interpolation\n",
    "image = original_image.resize((28, 28), resample=Image.BILINEAR)\n",
    "image.show()\n",
    "print(image)\n",
    "\n",
    "img = np.array(image)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must process an image. We get it as a PIL object and convert it to greyscale (going from shape (x, y, 4) in RGBA to (x, y)). Then, we resize the image to be of size (28, 28) using PIL's resize function and convert the PIL object to a numpy array. The cnn was trained on inversed images (background is black, digits are white) so we invert the images. We normalize them to tak evalues between 0 and 1. We then reshape this into a (1, 28, 28) tensor so that our cnn can handle this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_from_url(url):\n",
    "    img = Image.open(url).convert(\"L\")\n",
    "    img = img.resize((28, 28), resample=Image.BILINEAR)\n",
    "    img = np.array(img)\n",
    "    img = np.ones((28, 28)) * 255 - img\n",
    "    img = img / 255\n",
    "    img = np.reshape(img, (1, 28, 28))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are partially processed images that allows us to see what the images being fed into the neural network look like. Perhaps some flaws in the network's predictions come from these being off center? Consider addressing this by somehow centering the images, or by training on synthetic data. It does seem to process the images pretty well tho, so there aren't any noticeable problems that arise from resizing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partially_process(url):\n",
    "    img = Image.open(url).convert(\"L\")\n",
    "    img = img.resize((28, 28), resample=Image.BILINEAR)\n",
    "    img = np.array(img)\n",
    "    img = np.ones((28, 28)) * 255 - img\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_image(img):\n",
    "    while np.sum(img[0]) == 0:\n",
    "        img = img[1:]\n",
    "    while np.sum(img[:, 0]) == 0:\n",
    "        img = np.delete(img, 0, 1)\n",
    "    while np.sum(img[-1]) == 0:\n",
    "        img = img[:-1]\n",
    "    while np.sum(img[:, -1]) == 0:\n",
    "        img = np.delete(img, -1, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    resized_image = pil_image.resize(target_size)\n",
    "    resized_array = np.array(resized_image)\n",
    "\n",
    "    return resized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(image, target_size):\n",
    "    rows, cols = image.shape\n",
    "    target_rows, target_cols = target_size\n",
    "    \n",
    "    pad_row = (target_rows - rows) // 2\n",
    "    pad_col = (target_cols - cols) // 2\n",
    "    \n",
    "    padded_image = np.zeros(target_size)\n",
    "    padded_image[pad_row:pad_row+rows, pad_col:pad_col+cols] = image\n",
    "    \n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_center_of_mass(image):\n",
    "    rows, cols = image.shape\n",
    "    total_mass = np.sum(image)\n",
    "    row_indices, col_indices = np.indices((rows, cols))\n",
    "    \n",
    "    center_row = np.sum(row_indices * image) / total_mass\n",
    "    center_col = np.sum(col_indices * image) / total_mass\n",
    "    \n",
    "    return center_row, center_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_image(image):\n",
    "    rows, cols = image.shape\n",
    "    center_row, center_col = find_center_of_mass(image)\n",
    "    \n",
    "    shift_row = rows // 2 - int(center_row)\n",
    "    shift_col = cols // 2 - int(center_col)\n",
    "    \n",
    "    centered_image = np.roll(image, shift_row, axis=0)\n",
    "    centered_image = np.roll(centered_image, shift_col, axis=1)\n",
    "    \n",
    "    return centered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zero = partially_process(\"../images/0.png\")\n",
    "one = partially_process(\"../images/1.png\")\n",
    "two = partially_process(\"../images/2.png\")\n",
    "three = partially_process(\"../images/3.png\")\n",
    "four = partially_process(\"../images/4.png\")\n",
    "five = partially_process(\"../images/5.png\")\n",
    "six = partially_process(\"../images/6.png\")\n",
    "seven = partially_process(\"../images/7.png\")\n",
    "eight = partially_process(\"../images/8.png\")\n",
    "nine = partially_process(\"../images/9.png\")\n",
    "arr = [zero, one, two, three, four, five, six, seven, eight, nine]\n",
    "\n",
    "for i in arr:\n",
    "    Image.fromarray(i).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "five_test = center_image(five)\n",
    "five_test2 = shrink_image(five)\n",
    "five_test3 = resize_image(five_test2, (20, 20))\n",
    "a = center_image(five_test3)\n",
    "b = add_padding(a, (28, 28))\n",
    "five_test4 = add_padding(five_test3, (28, 28))\n",
    "five_test5 = center_image(five_test4)\n",
    "Image.fromarray(five).show()\n",
    "Image.fromarray(five_test).show()\n",
    "Image.fromarray(five_test2).show()\n",
    "Image.fromarray(five_test3).show()\n",
    "Image.fromarray(five_test4).show()\n",
    "Image.fromarray(five_test5).show()\n",
    "Image.fromarray(b).show()\n",
    "print(five_test4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on some manually drawn digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = process_from_url(\"../images/0.png\")\n",
    "one = process_from_url(\"../images/1.png\")\n",
    "two = process_from_url(\"../images/2.png\")\n",
    "three = process_from_url(\"../images/3.png\")\n",
    "four = process_from_url(\"../images/4.png\")\n",
    "five = process_from_url(\"../images/5.png\")\n",
    "six = process_from_url(\"../images/6.png\")\n",
    "seven = process_from_url(\"../images/7.png\")\n",
    "eight = process_from_url(\"../images/8.png\")\n",
    "nine = process_from_url(\"../images/9.png\")\n",
    "arr = [zero, one, two, three, four, five, six, seven, eight, nine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "3\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "5\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "6\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "7\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "6\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in arr:\n",
    "    print(np.argmax(model.predict(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1ba5e19180fac51b9befda9a64ad40cb64b262070331b52251e8022b1d0e710"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
